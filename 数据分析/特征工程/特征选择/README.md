# 数据规约

数据归约技术可以用来得到数据集的归约表示，它小得多，但仍接近地保持原数据的完整性。 这样，在归约后的数据集上挖掘将更有效，并产生相同(或几乎相同)的分析结果。用于数据分析的数据可能包含数以百计的属性，其中大部分属性与挖掘任务不相关，是冗余的。维度归约通过删除不相关的属性，来减少数据量，并保证信息的损失最小。

# 属性子集选择

目标是找出最小属性集，使得数据类的概率分布尽可能地接近使用所有属性的原分布。在压缩 的属性集上挖掘还有其它的优点。它减少了出现在发现模式上的属性的数目，使得模式更易于理解。

- 逐步向前选择：该过程由空属性集开始，选择原属性集中最好的属性，并将它添加到该集合中。在其后的每一次迭代，将原属性集剩下的属性中的最好的属性添加到该集合中。

- 逐步向后删除：该过程由整个属性集开始。在每一步，删除掉尚在属性集中的最坏属性。

- 向前选择和向后删除的结合：向前选择和向后删除方法可以结合在一起，每一步选择一个最 好的属性，并在剩余属性中删除一个最坏的属性。

Python scikit-learn 中的递归特征消除算法Recursive feature elimination (RFE)，就是利用这样的思想进行特征子集筛选的，一般考虑建立SVM或回归模型。

# 单变量重要性

分析单变量和目标变量的相关性，删除预测能力较低的变量。这种方法不同于属性子集选择，通常从统计学和信息的角度去分析。

- pearson相关系数和卡方检验，分析目标变量和单变量的相关性。

- 回归系数：训练线性回归或逻辑回归，提取每个变量的表决系数，进行重要性排序。

- 树模型的Gini指数：训练决策树模型，提取每个变量的重要度，即Gini指数进行排序。

- Lasso正则化：训练回归模型时，加入L1正则化参数，将特征向量稀疏化。

- IV指标：风控模型中，通常求解每个变量的IV值，来定义变量的重要度，一般将阀值设定在0.02以上。

通常的做法是根据业务需求来定，如果基于业务的用户或商品特征，需要较多的解释性，考虑采用统计上的一些方法，如变量的分布曲线，直方图等，再计算相关性指标，最后去考虑一些模型方法。如果建模需要，则通常采用模型方法去筛选特征，如果用一些更为复杂的GBDT，DNN等模型，建议不做特征选择，而做特征交叉。