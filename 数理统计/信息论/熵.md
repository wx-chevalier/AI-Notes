# 熵

信息熵反应了一个系统的有序化程度，一个系统越是有序，那么它的信息熵就越低，反之就越高。对于事件 $X=x$，定义自信息 Self-Information 为：$I(x)=-\log P(x)$。自信息仅仅处理单个输出，而熵就是为自信息的期望：

$$
H(X)=\mathbb{E}_{X \sim P(X)}[I(x)]=-\mathbb{E}_{X \sim P(X)}[\log P(x)]
$$

熵一般记作 $H(P)$。熵刻画了按照真实分布 来识别一个样本所需要的编码长度的期望（即平均编码长度），譬如含有 4 个字母 `(A,B,C,D)` 的样本集中，真实分布 $P=\left(\frac{1}{2}, \frac{1}{2}, 0,0\right)$，则只需要 1 位编码即可识别样本。对于离散型随机变量 $X$ ，假设其取值集合大小为 $K$ ，则可以证明：$0 \leq H(X) \leq \log K$ 。

# 条件熵

对于随机变量 $Y$ 和 $X$ ，条件熵 $H(Y|X)$ 表示：已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。条件熵的定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的期望：

$$
H(Y | X)=\mathbb{E}_{X \sim P(X)}[H(Y | X=x)]=-\mathbb{E}_{(X, Y) \sim P(X, Y)} \log P(Y | X)
$$

对于离散型随机变量，存在：

$$
H(Y | X)=\sum_{x} p(x) H(Y | X=x)=-\sum_{x} \sum_{y} p(x, y) \log p(y | x)
$$

对于连续型随机变量，则存在：

$$
H(Y | X)=\int p(x) H(Y | X=x) d x=-\iint p(x, y) \log p(y | x) d x d y
$$

根据定义可以证明：

$$
H(X, Y)=H(Y | X)+H(X)
$$

即：描述 $X$ 和 $Y$ 所需要的信息是：描述 $X$ 所需要的信息加上给定 $X$ 条件下描述 $Y$ 所需的额外信息。
