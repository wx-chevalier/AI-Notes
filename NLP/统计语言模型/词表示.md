

# Natural Language Processing
———这里是正式回答的分割线————

自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：

1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。

2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。

3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。

4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。

5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。

6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。

7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。自然语言处理经历了从规则的方法到基于统计的方法。基于统计的自然语言处理方法，在数学模型上和通信就是相同的，甚至相同的。但是科学家们也是用了几十年才认识到这个问题。统计语言模型的初衷是为了解决语音识别问题，在语音识别中，计算机需要知道一个文字序列能否构成一个有意义的句子。

**简单**

- 拼写检查
- 关键字搜索
- 查找同义词

**中等难度**

- 从网络或文档中提取信息

**难**

- 机器翻译（号称自然语言领域的圣杯）
- 语义分析（一句话是什么意思）
- 交叉引用（一句话中，他，这个等代词所对应的主体是哪个）
- 问答系统（Siri, Google Now, 小娜等）

## Reference

### Practices & Resources


### Books & Tools


- [数学之美]()


# Word Representation:词表示

自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。

## 词典
现实生活中，我们通过查词典来知道一个词的意思，这实际上是用另外的词或短语来表达一个词。这一方法在计算机领域也有，比如 [WordNet](http://wordnet.princeton.edu/) 实际上就是个电子化的英语词典。

然而，这一方式有以下几个问题：

- 有大量的同义词，不利于计算
- 更新缓慢，没有办法自动地添加新词
- 一个词释义含有比较明显的主观色彩
- 需要人工来创建和维护
- 很难计算词的相似性
- 很难进行计算，因为计算机本质上只认识 0 和 1


## One-hot Representation:基于统计的词语向量表达

NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。
比如，在一个精灵国里，他们的语言非常简单，总共只有三句话：

1. I like NLP.
2. I like deep learning.
3. I enjoy flying.

这样，我们可以看到这个精灵国的词典是 [I, like, NLP, deep, learning, enjoy, flying, .]。没错，我们把标点也认为是一个词。用向量来表达词时，我们创建一个向量，向量的维度与词典的个数相同，然后让向量的某个位置为 1 ，其他位置全为 0。这样就创建了一个向量词 (one-hot)。

比如，在我们的精灵国里，I 这个词的向量是：[1 0 0 0 0 0 0 0], deep 这个词的向量表达是 [0 0 0 1 0 0 0 0]。

看起来挺好，我们终于把词转换为 0 和 1 这种计算机能理解的格式了。然而，这种表达也有个问题，很多同义词没办法表达出来，因为他们是不同的向量。怎么解决这个问题呢？我们可以通过词的上下文来表达一个词。通过上下文表达一个词的另外一个好处是，一个词往往有多个意思，具体在某个句子里是什么意思往往由它的上下文决定。

　　这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 

ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 

表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

　　当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。

## 基于上下文的表达
> You shall know a word by the company it keeps. --- (J. R. Firth 1957: 11)



词向量(Distributed Representation)

而是用 **Distributed Representation**（不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念）表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177,−0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。（个人认为）Distributed representation 

最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

# Document Representation(文档表示)
## Bag-of-Words


BOW (bag of words) 模型简介 Bag of words模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子（因为里面装的都是词汇，所以称为词袋，Bag of words即因此而来），然后看这个袋子里装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。举个例子，有如下两个文档：

文档一：Bob likes to play basketball, Jim likes too.

文档二：Bob also likes to play football games.

BOW不仅是直观的感受，其数学原理还依托于离散数学中的[多重集](https://zh.m.wikipedia.org/wiki/%E5%A4%9A%E9%87%8D%E9%9B%86)这个概念，多重集或多重集合是数学中的一个概念，是集合概念的推广。在一个集合中，相同的元素只能出现一次，因此只能显示出有或无的属性。在多重集之中，同一个元素可以出现多次。正式的多重集的概念大约出现在1970年代。多重集的势的计算和一般集合的计算方法一样，出现多次的元素则需要按出现的次数计算，不能只算一次。一个元素在多重集里出现的次数称为这个元素在多重集里面的重数（或重次、重复度）。举例来说， {1,2,3} 是一个集合，而 {\displaystyle \left\{1,1,1,2,2,3\right\}}  不是一个集合，而是一个多重集。其中元素1的重数是3，2的重数是2，3的重数是1。 {\displaystyle \left\{1,1,1,2,2,3\right\}}  的元素个数是6。有时为了和一般的集合相区别，多重集合会用方括号而不是花括号标记，比如 {\displaystyle \left\{1,1,1,2,2,3\right\}}  会被记为 {\displaystyle \left[1,1,1,2,2,3\right]}  。和多元组或数组的概念不同，多重集中的元素是没有顺序分别的，也就是说 {\displaystyle \left[1,1,1,2,2,3\right]}  和 {\displaystyle \left[1,1,2,1,2,3\right]}  是同一个多重集。

基于这两个文本文档，构造一个词典：

Dictionary = {1:”Bob”, 2. “like”, 3. “to”, 4. “play”, 5. “basketball”, 6. “also”, 7. “football”，8. “games”, 9. “Jim”, 10. “too”}。

这个词典一共包含10个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个10维向量表示（用整数数字0~n（n为正整数）表示某个单词在文档中出现的次数）：

1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]

2：[1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]

向量中每个元素表示词典中相关元素在文档中出现的次数(下文中，将用单词的直方图表示)。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序（这是本Bag-of-words模型的缺点之一，不过瑕不掩瑜甚至在此处无关紧要）。








# Distributed Representation:分布式表示

分布式表示的

某个词的含义可以由其所处上下文中的其他词推导而来，譬如在 `saying that Europe needs unified banking regulation to replace the hodgepodge` 与 `government debt problems turning into banking cries as has happened in`
