# 目标函数

## 目标函数

## Masked Language Model, MLM

MLM 是为了训练深度双向语言表示向量，BERT 用了一个非常直接的方式，遮住句子里某些单词，让编码器预测这个单词是什么。具体操作流程如下图示例：先按一个较小概率 mask 掉一些字，再对这些字利用语言模型由上下文做预测。

![](https://i.postimg.cc/85DMhMtm/image.png)

BERT 具体训练方法为：随机遮住 15% 的单词作为训练样本，其中 80%用 masked token 来代替；10% 用随机的一个词来替换，10% 保持这个词不变。

直观上来说，只有 15%的词被遮盖的原因是性能开销，双向编码器比单向编码器训练要慢；选 80%mask，20%具体单词的原因是在 pretrain 的时候做了 mask，在特定任务微调如分类任务的时候，并不对输入序列做 mask，会产生 gap，任务不一致；10%用随机的一个词来替换，10%保持这个词不变的原因是让编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个 token 的表示向量，做了一个折中。

![](https://i.postimg.cc/rmJWgsHX/image.png)

预训练一个二分类的模型，来学习句子之间的关系。预测下一个句子的方法对学习句子之间关系很有帮助。

训练方法：正样本和负样本比例是 1：1，50% 的句子是正样本，即给定句子 A 和 B，B 是 A 的实际语境下一句；负样本：在语料库中随机选择的句子作为 B。通过两个特定的 token\[CLS\] 和 \[SEP\] 来串接两个句子，该任务在\[CLS\]位置输出预测。

![](https://i.postimg.cc/90Q9kV34/image.png)

