
# 浅层神经网络


# 单隐层计算


![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2017/8/3/QQ20170904-223601111.png) 


![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2017/8/3/QQ20170904-230027.png) 


我们可以得到结构相似的输入数据与一层隐层输出


![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2017/8/3/Group.png)



![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2017/8/3/QQ20170904-231536.png) 


# Activation Function


构建神经网络中非常重要的一个环节就是选择合适的激活函数（Activation Function），激活函数是为了增加神经网络模型的非线性，也可以看做从数据空间到最终表达空间的一种映射。仅就 Sigmod 与 tahn 相比时，在大部分情况下我们应该优先使用 tahn 函数；除了在最终的输出层，因为输出层我们需要得到的是 0~1 范围内的概率表示。譬如在上面介绍的浅层神经网络中，我们就可以使用 Sigmod 作为隐层的激活函数，而使用 tahn 作为输出层的激活函数。


不过 Sigmod 与 tahn 同样都存在在极大值或者极小值处梯度较小、收敛缓慢的问题。并且采用 Sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大；而采用 ReLU(rectified linear unit) 激活函数，整个过程的计算量节省很多。此外，ReLU 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。


![](https://raw.githubusercontent.com/wxyyxc1992/OSS/master/2017/8/1/activation_function.png)



## 为什么需要非线性激活函数？


如果我们选择了所谓的 Identity Activation Function，即直接将输入值作为输出返回，这样我们最终的输出值也就自然与输入值存在线性相关性。


## 激活函数的导数


# 梯度下降（Gradient Descent）


## 反向传播的直观解释


## 随机初始化


# 延伸阅读