# 机器人问答

当用户询问了机器人，机器人会根据聊天上下文，到知识库里寻找最合适的解决方案，并将答案返回给用户。如果机器人解决不了，则会由人工进行处理。机器人问答常用的组织形式有 FAQ（非结构化），KB（结构化）两类。我们这里讲的定位是 FAQ 形式。

知识库首先由很多的 FAQ 构成，比如构建一个客服领域的知识库，就需要将客服整个垂直场景涉及的问题都罗列出来，并配置相应的答案。比如，“我要怎么开淘宝店？”，“我要怎么退款”，“我的密码要怎么重置”。成千上万的 FAQ 对，构成了整个知识库的基础。机器人在回答时，就是从知识库里找到和用户问题非常接近的标准问题（称为“知识”），用它的答案进行回复。

![](https://i.postimg.cc/6pf44tSL/image.png)

# 知识库的组织

知识库的逻辑构成如下，3 层结构：标准问题，相似问题，标准答案。一个标准问题对应一个标准答案，一个标准问题下有多个相似问题。机器人定位时，使用标准问题和相似问题进行定位。

![](https://i.postimg.cc/Hkzjs3c6/image.png)

# 算法架构

一般的机器人问答链路中我们主要会从不同的链路去生成结果：

- 检索链路：BERT, HCNN, OpenSearch
- 生成链路：Transformer
- 规则链路：Tire 树，基于依存句法的生成
- 辅助算法：敏感词过滤，语言模型，关键词聚类

![](https://i.postimg.cc/632QcXHP/image.png)

## 检索链路

### 模糊搜索

检索链路首先是从 OpenSearch 中进行搜索，我们的检索库是定期更新数据，数据通过 ODPS 进行处理，从 ODPS 导入 OpenSearch，没有实时增量，所以 OpenSearch 的基础能力满足我们的需求。公开语料库譬如百度知道，全量 4.5 亿调。语料都通过 ODPS 的 UDF，先进行语料清洗，再进行语料去重。

索引构建阶段，使用 alinlp 电商分词的结果进行索引构建，对名词和动词做了单独处理。使用 alinlp 电商分词后，构建搜索表达式，增加名词和动词的权重，通过 OpenSearch 进行搜索。

### HCNN 精排

精排过程就是比较两个句子的相似度，比较方式一般有两种，Sentence Interaction 和 Sentence Embedding。SE 就是将两个句子变成同一空间里的独立向量，然后计算这两个向量之间的余弦相似度。典型代表如：DSSM，ABCNN。SI 就是将两个句子进行交叉，比如使用向量构造矩阵，通过对矩阵的理解，得到句子之间的相似度。典型代表如：Pyramid。

HCNN 是 Hybrid CNN 的缩写，它包括了 SE 和 SI，分别构造左右两个子网络，一个是 SI，一个是 SE，把两种方式进行了结合。

![](https://i.postimg.cc/pXx2hzt4/image.png)

### BERT 精排

BERT 的信息抽取器是 Transformer，Transformer 在翻译的任务里就表现出了极强的信息抽取能力，再经过大量数据进行训练，我们相信 BERT 能够比 HCNN 有更好的效果。

## 生成链路

针对生成问题，由于是有无到有，而且整个模型是端到端生成，人工可以干预的地方并不多，对整个模型的控制几种在模型设计、超参配置和训练过程中。

在开发阶段，我们尝试了 Seq2Seq、ConSeq2Seq，以上模型经常生成 save answer 和不通顺的语句，场景建模能力较弱，非该场景的生成语句偏多。在尝试了 Transformer + Beam Search 的架构后，Transformer 本身的并行化特性，以及网络结构里，Multi-head Attention 能够获取到更丰富语义，生成效率和生成句子的多样性以及句子质量都得到了答复提高。

![](https://i.postimg.cc/Mp3xFLjJ/image.png)

## 规则链路

### Trie 树

使用 Trie 树替换原始句子里的同义词。原始 Trie 树包含了模糊匹配、语义节点、集合词、同义词等，目的是为了扩大覆盖，语义归一。但是我们的需求是替换原始句子的同义词，所以很多功能用不上。考虑到修改原始 Trie 树的成本比较大，于是写了 MiniTrie 树，只做同义词替换。

MiniTrie 先读取 Trie 树的同义词文件，在内存里建立替换关系图，然后对输入的 Query 可以进行替换，输出结果。MiniTrie 在同义词替换时，支持最小匹配、最大匹配、全匹配三种匹配方式，通过参数进行配置。

### 基于依存句法的生成

![](https://i.postimg.cc/MTD99Cvy/image.png)

将 root 出发的子树进行合并，调整/删除子树，构造新的句子。整个流程包括两部分，训练和预测。训练是依赖于相似的句对，构造可转换的规则库。

1）确保相似句对来自同一个领域

2）使用 Alinlp 对句对分别进行依存句法分析，获得 chunk 序列，包含依存关系及相应的 label

3）使用 chunk 合并器，对生成的 chunk 序列进行合并

4）取 chunk 序列的 label 作为句子表示，将句对的 label 序列作为一条规则加入到规则库

预测阶段通过规则库，对转换后的句子进行筛选：

1）使用 Alinlp 对输入语句进行依存句法分析，获得 chunk 序列，包含依存关系及相应的 label

2）使用 chunk 合并器，对生成的 chunk 序列进行合并

3）对 chunk 序列进行位置置换，或者删除 chunk，生成候选集

4）对候选集中的 chunk 序列，取 label 作为表示，用规则库判断转换是否合理，不合理的则丢弃

![](https://i.postimg.cc/FsQjYWzv/image.png)

## 辅助算法

### 语言模型

多层双向 LSTM，使用淘系语料进行训练

### 聚类

基于 TextRank 的关键词聚类

### 敏感词过滤

基于 KFC 的 AC 自动机和双数组 trie 树的关键词过滤
