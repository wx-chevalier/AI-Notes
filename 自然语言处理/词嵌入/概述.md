预训练的词向量已经引领自然语言处理很长时间。Word2vec[4] 在 2013 年被作为一个近似的语言建模模型而提出。当时，硬件速度比现在要慢很多，并且深度学习模型也还没有得到广泛的支持，Word2vec 凭借着自身的效率和易用性被采用。从那时起，实施 NLP 项目的标准方法基本上就没变过：通过 Word2vec 和 GloVe[5] 等算法在大量未标注的数据上进行预训练获得词嵌入向量 (word embedding)，然后把词嵌入向量用于初始化神经网络的第一层，而网络的其它部分则是根据特定的任务，利用其余的数据进行训练。在大多数训练数据有限的任务中，这种做法能够使准确率提升 2 到 3 个百分点 [6]。不过，尽管这些预训练的词嵌入向量具有极大的影响力，但是它们存在一个主要的局限：它们只将先前的知识纳入模型的第一层，而网络的其余部分仍然需要从头开始训练。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1fz3vrajfpbj30u00aldj6.jpg)

由 word2vec 捕捉到的关系（来源：TensorFlow 教程）
Word2vec 以及相关的其它方法属于浅层方法，这是一种以效率换表达力的做法。使用词嵌入向量就像使用仅对图像边缘进行编码的预训练表征来初始化计算机视觉模型，尽管这种做法对许多任务都是有帮助的，但是却无法捕捉到那些也许更有用的高层次信息。采用词嵌入向量初始化的模型需要从头开始学习，模型不仅要学会消除单词歧义，还要理解单词序列的意义。这是语言理解的核心内容，它需要对复杂的语言现象建模，例如语义合成性（compositionality）、多义性（polysemy）、指代（anaphora）、长期依赖（long-term dependencies）、一致性（agreement）和否定（negation）等。因此，使用这些浅层表征初始化的自然语言处理模型仍然需要大量的训练样本，才能获得良好的性能。
